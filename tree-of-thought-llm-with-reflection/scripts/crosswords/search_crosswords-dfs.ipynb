{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (1.57.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/henryyi/opt/anaconda3/envs/ML-Algorithm-LowerVersion/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.0\n",
      "    Uninstalling openai-1.57.0:\n",
      "      Successfully uninstalled openai-1.57.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tree-of-thoughts-llm 0.1.0 requires openai==0.27.7, but you have openai 1.58.1 which is incompatible.\n",
      "tree-of-thoughts-llm 0.1.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "tree-of-thoughts-llm 0.1.0 requires sympy==1.12, but you have sympy 1.13.1 which is incompatible.\n",
      "tree-of-thoughts-llm 0.1.0 requires tqdm==4.65.0, but you have tqdm 4.67.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-1.58.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tot.prompts.crosswords import propose_prompt, value_prompt\n",
    "from tot.models import gpt\n",
    "from tot.tasks.crosswords import MiniCrosswordsEnv\n",
    "\n",
    "env = MiniCrosswordsEnv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\n",
      "\n",
      "Current Board:\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "\n",
      "Unfilled:\n",
      "h1. An agendum; something to be done: _____\n",
      "h2. An engine: _____\n",
      "h3. Pretentious; flowery: _____\n",
      "h4. A salon; a hall: _____\n",
      "h5. To mock; to sneer: _____\n",
      "v1. To heap: _____\n",
      "v2. An Indian antelope: _____\n",
      "v3. To intend; to plan; to devise; a nettle; to guess: _____\n",
      "v4. A nozzle: _____\n",
      "v5. Desiccator; more dry: _____\n",
      "\n",
      "Filled:\n",
      "\n",
      "Changed:\n",
      "\n",
      "\n",
      "Given the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prompt_wrap(obs):\n",
    "    return propose_prompt.format(input=obs)\n",
    "\n",
    "print(prompt_wrap(env.reset(0)))\n",
    "# print('---------')\n",
    "# print(prompt_wrap(env.step('h2. value')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from tot.models import gpt\n",
    "\n",
    "def parse_line(input_str):\n",
    "    # regular expression pattern to match the input string format\n",
    "    pattern = r'^([hv][1-5])\\. ([a-zA-Z]{5,5}) \\((certain|high|medium|low)\\).*$'\n",
    "\n",
    "    # use regex to extract the parts of the input string\n",
    "    match = re.match(pattern, input_str)\n",
    "\n",
    "    if match:\n",
    "        # extract the matched groups\n",
    "        parts = [match.group(1), match.group(2), match.group(3)]\n",
    "        return parts\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "confidence_to_value = {'certain': 1, 'high': 0.5, 'medium': 0.2, 'low': 0.1}  # TODO: ad hoc\n",
    "\n",
    "def parse_response(response):\n",
    "    # split the response into lines\n",
    "    lines = response.split('\\n')\n",
    "\n",
    "    # parse each line\n",
    "    parsed_lines = [parse_line(line) for line in lines]\n",
    "\n",
    "    # filter out the lines that didn't match the format\n",
    "    parsed_lines = [(line[0].lower() + '. ' + line[1].lower(), confidence_to_value.get(line[2], 0)) for line in parsed_lines if line is not None]\n",
    "\n",
    "    return parsed_lines if len(parsed_lines) >= 1 else None\n",
    "\n",
    "\n",
    "def get_candidates_to_scores(env):\n",
    "    obs = env.render()\n",
    "    if obs in env.cache: \n",
    "        print('cache hit')\n",
    "        return env.cache[obs]\n",
    "    print('call gpt')\n",
    "    responses = gpt(prompt_wrap(obs), model='gpt-3.5-turbo', n=8)\n",
    "    candidates_to_scores = {}\n",
    "    for response in responses:\n",
    "        parsed_response = parse_response(response)\n",
    "        if parsed_response:\n",
    "            for candidate, score in parsed_response:\n",
    "                candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score\n",
    "        # choose candiate with highest score\n",
    "    # print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    env.cache[obs] = candidates_to_scores\n",
    "    return candidates_to_scores\n",
    "\n",
    "def propose_score(env, idx):\n",
    "    obs = env.reset(idx)\n",
    "    done = False\n",
    "    infos = []\n",
    "    while not done:\n",
    "        responses = gpt(prompt_wrap(obs), model='gpt-3.5-turbo', n=5)\n",
    "        candidates_to_scores = {}\n",
    "        for response in responses:\n",
    "            parsed_response = parse_response(response)\n",
    "            if parsed_response:\n",
    "                for candidate, score in parsed_response:\n",
    "                    candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score\n",
    "        # choose candiate with highest score\n",
    "        print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(candidates_to_scores) == 0:\n",
    "            break\n",
    "        candidates =  sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=True)\n",
    "        for candidate in candidates:\n",
    "            env_ = copy.deepcopy(env)\n",
    "            env_.step(candidate)\n",
    "            if not any(_ == 2 for _ in env_.status):\n",
    "                break\n",
    "        print(candidate)\n",
    "        # candidate = input()\n",
    "        obs, r, done, info = env.step(candidate)\n",
    "        print(obs)\n",
    "        print(env.steps, info)\n",
    "        print('-------------------\\n\\n\\n')\n",
    "        infos.append(info)\n",
    "    return infos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(env, actions, infos, time_limit, prune, max_per_state):\n",
    "    # get candidate thoughts\n",
    "    candidates_to_scores = get_candidates_to_scores(env)\n",
    "    if len(candidates_to_scores) == 0: return 0, [], []\n",
    "    print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # back up current state\n",
    "    board, status, steps = env.board.copy(), env.status.copy(), env.steps\n",
    "\n",
    "    # try each candidate\n",
    "    cnt_per_state = 0\n",
    "    for action in sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=True):\n",
    "        obs, r, done, info = env.step(action)\n",
    "        r = info['r_word']\n",
    "        if len(infos) < time_limit and env.steps < 10 and not any(_ == 2 for _ in env.status):  # not violating any existing constraints\n",
    "            cnt_per_state += 1\n",
    "            if cnt_per_state > max_per_state: break\n",
    "            count = env.prompt_status()       \n",
    "            actions.append(action)  \n",
    "\n",
    "            print(len(infos))\n",
    "            print(actions)\n",
    "            print(env.render_board())\n",
    "            print(info)\n",
    "            print(count)\n",
    "            if infos:\n",
    "                best = max(infos, key=lambda x: x['info']['r_word'])\n",
    "                print('best', best)\n",
    "            print('--------------')\n",
    "            print()\n",
    "\n",
    "            info = {'total_step': len(infos), 'env_step': env.steps, 'actions': actions.copy(), 'info': info, 'count': count}\n",
    "            infos.append(info)\n",
    "            if not prune or count['impossible'] < 1:  # only continue if the current status is possible\n",
    "                dfs(env, actions, infos, time_limit, prune, max_per_state)\n",
    "            actions.pop()\n",
    "        env.reset(env.idx, board=board.copy(), status=status.copy(), steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import random\n",
    "def crossword_execute(env, action_info, actions, other_params):\n",
    "    action = action_info['action']\n",
    "    if action_info['env_info'] is not None:\n",
    "        board, status, steps = action_info['env_info']\n",
    "        env.reset(env.idx, board=board.copy(), status=status.copy(), steps=steps)\n",
    "    obs, r, done, info = env.step(action)\n",
    "    r = info['r_word']\n",
    "    if len(other_params['infos']) < other_params['time_limit'] and env.steps < 10 and not any(_ == 2 for _ in env.status):  # not violating any existing constraints\n",
    "        \n",
    "        count = env.prompt_status()\n",
    "        actions.append(action)  \n",
    "\n",
    "        print(len(other_params['infos']))\n",
    "        print(actions)\n",
    "        print(env.render_board())\n",
    "        print(info)\n",
    "        print(count)\n",
    "        if other_params['infos']:\n",
    "            best = max(other_params['infos'], key=lambda x: x['info']['r_word'])\n",
    "            print('best', best)\n",
    "        print('--------------')\n",
    "        print()\n",
    "\n",
    "        info = {'total_step': len(other_params['infos']), 'env_step': env.steps, 'actions': actions.copy(), 'info': info, 'count': count}\n",
    "        other_params['infos'].append(info)\n",
    "        if other_params['prune'] and count['impossible'] >= 1:  # only continue if the current status is possible\n",
    "            return 'non-generate'\n",
    "    return 'generate'\n",
    "\n",
    "def crossword_generate(env, action_info, other_params):\n",
    "    candidates_to_scores = get_candidates_to_scores(env)\n",
    "    if len(candidates_to_scores) == 0: return []\n",
    "    sorted_candidates = sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=False)\n",
    "    sorted_scores = [candidates_to_scores[candidate] for candidate in sorted_candidates]\n",
    "    print(other_params['max_per_state'])\n",
    "    if other_params['max_per_state'] is not None and other_params['max_per_state'] > 0:\n",
    "        sorted_candidates = sorted_candidates[:other_params['max_per_state']]\n",
    "        sorted_scores = sorted_scores[:other_params['max_per_state']]\n",
    "    print({ \"result_list\": [{\"action\": sorted_candidate, \"env_info\": (env.board.copy(), env.status.copy(), env.steps)} for sorted_candidate in sorted_candidates], \"priority_list\": sorted_scores })\n",
    "    return { \"result_list\": [{\"action\": sorted_candidate, \"env_info\": (env.board.copy(), env.status.copy(), env.steps)} for sorted_candidate in sorted_candidates], \"priority_list\": sorted_scores }\n",
    "\n",
    "def sliding_window_sanity_check(sliding_window_size, length_of_queue_stack):\n",
    "    processed_sliding_window_size = [0, 0]\n",
    "    if sliding_window_size[0] < 0:\n",
    "        if sliding_window_size[0] < -length_of_queue_stack:\n",
    "            processed_sliding_window_size[0] = -length_of_queue_stack\n",
    "        else:\n",
    "            processed_sliding_window_size[0] = sliding_window_size[0]\n",
    "    else:\n",
    "        if sliding_window_size[0] >= length_of_queue_stack:\n",
    "            processed_sliding_window_size[0] = length_of_queue_stack - 1\n",
    "        else:\n",
    "            processed_sliding_window_size[0] = sliding_window_size[0]\n",
    "\n",
    "    if sliding_window_size[1] < 0:\n",
    "        if sliding_window_size[1] < -length_of_queue_stack:\n",
    "            processed_sliding_window_size[1] = -length_of_queue_stack\n",
    "        else:\n",
    "            processed_sliding_window_size[1] = sliding_window_size[1]\n",
    "    else:\n",
    "        if sliding_window_size[1] >= length_of_queue_stack:\n",
    "            processed_sliding_window_size[1] = length_of_queue_stack - 1\n",
    "        else:\n",
    "            processed_sliding_window_size[1] = sliding_window_size[1]\n",
    "    return processed_sliding_window_size\n",
    "\n",
    "def auto_search(env, other_params, execute_func, generate_func, epsilon = 0.3, decay_rate = 0.9, sliding_window_size = None, heapify_queue_stack = False, queue_stack_valuate_func = None):\n",
    "    queue_stack = [{'action': None, 'parent_actions': [], 'env_info': None, 'level': 0}]\n",
    "    while queue_stack:\n",
    "        if queue_stack_valuate_func is not None:\n",
    "            queue_stack_valuate_func(env, queue_stack, other_params)\n",
    "        random_number = random.random()\n",
    "        if random_number < epsilon:\n",
    "            sliding_window_indexes = (0, len(queue_stack) - 1) if sliding_window_size is None else sliding_window_size\n",
    "            if heapify_queue_stack:\n",
    "                queue_stack_copy = queue_stack.copy()\n",
    "                heapq.heapify(queue_stack_copy)\n",
    "                processed_sliding_window_indexes = sliding_window_sanity_check(sliding_window_indexes, len(queue_stack_copy))\n",
    "                action_info = queue_stack_copy.pop(random.randint(processed_sliding_window_indexes[0], processed_sliding_window_indexes[1]))\n",
    "                queue_stack.pop(queue_stack.index(action_info))\n",
    "            else:\n",
    "                processed_sliding_window_indexes = sliding_window_sanity_check(sliding_window_indexes, len(queue_stack))\n",
    "                action_info = queue_stack.pop(random.randint(processed_sliding_window_indexes[0], processed_sliding_window_indexes[1]))\n",
    "        else:\n",
    "            action_info = queue_stack.pop()\n",
    "        actions = action_info['parent_actions'].copy()\n",
    "        level = action_info['level']\n",
    "\n",
    "        to_generate_thoughts = 'generate'\n",
    "\n",
    "        if action_info['action'] is not None:\n",
    "            to_generate_thoughts = execute_func(env, action_info, actions, other_params)\n",
    "\n",
    "        if to_generate_thoughts == 'generate':\n",
    "            new_thoughts = generate_func(env, action_info, other_params)\n",
    "            if (type(new_thoughts) == dict) and 'priority_list' in new_thoughts and new_thoughts['priority_list'] is not None:\n",
    "                if len(new_thoughts['result_list']) == 0: continue\n",
    "                for priority, action_info in zip(new_thoughts['priority_list'], new_thoughts['result_list']):\n",
    "                    queue_stack.append((\n",
    "                        priority, {\n",
    "                            'action': action_info['action'], \n",
    "                            'parent_actions': actions.copy(), \n",
    "                            'env_info': action_info['env_info'],\n",
    "                            'level': level + 1\n",
    "                        }\n",
    "                    ))\n",
    "            else:\n",
    "                if len(new_thoughts) == 0: continue\n",
    "                for action_info in new_thoughts:\n",
    "                    queue_stack.append({\n",
    "                        'action': action_info['action'],\n",
    "                        'parent_actions': actions.copy(), \n",
    "                        'env_info': action_info['env_info'],\n",
    "                        'level': level + 1\n",
    "                        })\n",
    "        elif to_generate_thoughts == 'non-generate':\n",
    "            continue\n",
    "        elif to_generate_thoughts == 'break':\n",
    "            break\n",
    "        epsilon = epsilon * decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call gpt\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Let\\'s play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\\n\\nCurrent Board:\\n_____\\n_____\\n_____\\n_____\\n_____\\n\\nUnfilled:\\nh1. An agendum; something to be done: _____\\nh2. An engine: _____\\nh3. Pretentious; flowery: _____\\nh4. A salon; a hall: _____\\nh5. To mock; to sneer: _____\\nv1. To heap: _____\\nv2. An Indian antelope: _____\\nv3. To intend; to plan; to devise; a nettle; to guess: _____\\nv4. A nozzle: _____\\nv5. Desiccator; more dry: _____\\n\\nFilled:\\n\\nChanged:\\n\\n\\nGiven the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\\n'}], 'temperature': 0.7, 'max_tokens': 1000, 'n': 8, 'stop': None}\n"
     ]
    }
   ],
   "source": [
    "# dfs with pruning\n",
    "infoss = []\n",
    "for i in range(0, 100, 5):\n",
    "    env.reset(i)\n",
    "    infos = []\n",
    "    actions = []\n",
    "    other_params = {'infos': infos, 'time_limit': 100, 'prune': True, 'max_per_state': 3}\n",
    "    auto_search(env, other_params, crossword_execute, crossword_generate, epsilon=0.3, decay_rate=0.9, sliding_window_size=(-3, -1), heapify_queue_stack=False)\n",
    "    infoss.append(infos)\n",
    "    with open('logs/crosswords/infoss_dfs_prune.json', 'w') as fout:\n",
    "        json.dump(infoss, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs with pruning\n",
    "infoss = []\n",
    "for i in range(0, 100, 5):\n",
    "    env.reset(i)\n",
    "    infos = []\n",
    "    actions = []\n",
    "    dfs(env, actions, infos, 100, prune=True, max_per_state=3)\n",
    "    infoss.append(infos)\n",
    "    with open('logs/crosswords/infoss_dfs_prune.json', 'w') as fout:\n",
    "        json.dump(infoss, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs with pruning\n",
    "infoss = []\n",
    "for i in range(0, 100, 5):\n",
    "    env.reset(i)\n",
    "    infos = []\n",
    "    actions = []\n",
    "    dfs(env, actions, infos, 100, prune=True, max_per_state=3)\n",
    "    infoss.append(infos)\n",
    "    with open('logs/crosswords/infoss_dfs_prune.json', 'w') as fout:\n",
    "        json.dump(infoss, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs without pruning\n",
    "infoss = []\n",
    "for i in range(0, 100, 5):\n",
    "    env.reset(i)\n",
    "    infos = []\n",
    "    actions = []\n",
    "    dfs(env, actions, infos, 100, prune=False, max_per_state=3)\n",
    "    infoss.append(infos)\n",
    "    with open('logs/crosswords/infoss_dfs_no_prune.json', 'w') as fout:\n",
    "        json.dump(infoss, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAHAHA 3\n",
      "HAHAHA 2\n",
      "HAHAHA 1\n"
     ]
    }
   ],
   "source": [
    "things = [1,2,3]\n",
    "while things:\n",
    "    print('HAHAHA', things.pop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sliding_window_size = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (3428478268.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    processed_sliding_window_size(0) = 100\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
